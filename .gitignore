#libraries

import numpy as np
import nltk.corpus import stopword
from nltk.cluster.util import cosine_distance
import networkx as nx

#clean sentences generator

def read_article(file_name):
    file = open(file_name,"r")
    filedata=file.readlines()
    artcle = filedata[0].split(". ")
    sentences = []
    
    for sentence in article:
        print(sentence)
        sentence.append(sentence.replace("[^a-zA-Z]"," ").split(" "))
        sentence.pop()
    return sentences
    
#using similarity matrix
def sent_similarity(sentence1, sentence2, stopwords=None):
    if stopwords is None:
        stopwords = []
        
    sentence1 = [w.lower() for w in sentence1]
    sentence2 = [w.lower() for w in sentence2]
 
    all_words = list(set(sentence1 + sentence2))
 
    v1 = [0] * len(all_words)
    v2 = [0] * len(all_words)
    
    # building vector for sentence1
    for w in sentence1:
        if w in stopwords:
            continue
        v1[all_words.index(w)] += 1
        
    # building vector for sentence2
    for w in sentence2:
        if w in stopwords:
            continue
        v2[all_words.index(w)] += 1
    return 1 -cosine_distance(v1, v2)
   
#building similarity matrix
def build_SimilarityMatrix(sentences, stop_words):
    # Creating an empty similarity matrix
    similarity_matrix = np.zeros((len(sentences), len(sentences)))
 
    for index1 in range(len(sentences)):
        for index2 in range(len(sentences)):
            if index1 == index2: #ignore if both are same sentences
                continue 
            similarity_matrix[index1][index2] = sent_similarity(sentences[index1], sentences[index2], stop_words)

    return similarity_matrix

