#libraries

import numpy as np
import nltk.corpus import stopword
from nltk.cluster.util import cosine_distance
import networkx as nx

#clean sentences generator

def read_article(file_name):
    file = open(file_name,"r")
    filedata=file.readlines()
    artcle = filedata[0].split(". ")
    sentences = []
    
    for sentence in article:
        print(sentence)
        sentence.append(sentence.replace("[^a-zA-Z]"," ").split(" "))
        sentence.pop()
    return sentences
    
#using similarity matrix
def sent_similarity(sentence1, sentence2, stopwords=None):
    if stopwords is None:
        stopwords = []
        
    sentence1 = [w.lower() for w in sentence1]
    sentence2 = [w.lower() for w in sentence2]
 
    all_words = list(set(sentence1 + sentence2))
 
    v1 = [0] * len(all_words)
    v2 = [0] * len(all_words)
    
    # building vector for sentence1
    for w in sentence1:
        if w in stopwords:
            continue
        v1[all_words.index(w)] += 1
        
    # building vector for sentence2
    for w in sentence2:
        if w in stopwords:
            continue
        v2[all_words.index(w)] += 1
    return 1 -cosine_distance(v1, v2)
