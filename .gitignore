#libraries

import numpy as np
from nltk.corpus import stopwords
from nltk.cluster.util import cosine_distance
import networkx

#clean sentences generator

def read_article(file_name):
    file = open(file_name,"r")
    filedata=file.readlines()
    article = filedata[0].split(". ")
    sentences = []
    
    for sentence in article:
        print(sentence)
        sentences.append(sentence.replace("[^a-zA-Z]", " ").split(" "))
     sentences.pop()
     return sentences
    
#using similarity matrix
def sent_similarity(sentence1, sentence2, stopwords=None):
    if stopwords is None:
        stopwords = []
        
    sentence1 = [w.lower() for w in sentence1]
    sentence2 = [w.lower() for w in sentence2]
 
    all_words = list(set(sentence1 + sentence2))
 
    v1 = [0] * len(all_words)
    v2 = [0] * len(all_words)
    
    # building vector for sentence1
    for w in sentence1:
        if w in stopwords:
            continue
        v1[all_words.index(w)] += 1
        
    # building vector for sentence2
    for w in sentence2:
        if w in stopwords:
            continue
        v2[all_words.index(w)] += 1
    return 1 -cosine_distance(v1, v2)
   
#building similarity matrix
def build_SimilarityMatrix(sentences, stop_words):
    # Creating an empty similarity matrix
    similarity_matrix = np.zeros((len(sentences), len(sentences)))
 
    for index1 in range(len(sentences)):
        for index2 in range(len(sentences)):
            if index1 == index2: #ignore if both are same sentences
                continue 
            similarity_matrix[index1][index2] = sent_similarity(sentences[index1], sentences[index2], stop_words)

    return similarity_matrix
    
#Here we well be using the code to generate the summary
def generate_summary(file_name,top_n = 5):
    stop_words = stopwords.words('english')
    summarize_text = []
     
    #step 1
    #reading news articles and splitig it in to the sentence
    sentences = read_article(file_name):
     
    #step2
    #generating similarity matrix across the sentences
    sentence_similarity_martix  = build_SimilarityMatrix(sentences, stop_words)
     
    #step3
    #Rank sentences in similarity martix
    sentence_similarity_graph = networkx.from_numpy_array(sentence_similarity_martix)
    scores = networkx.pagerank(sentence_similarity_graph)
     
    #step4
    #sort the rank and pick the top rank sentences
    ranked_sentence = sorted(((scores[i],s) for i,s in enumarate(senences)),reverse = True)
    print("Indexes of top ranked_sentence order are ", ranked_sentence)
    
    for i in range(top_n):
        summarize_text.append(" ".join(ranked_sentence[i][1]))
     
    #step5
    #output of summarize text
    print("Summarize Text: \n",". ".join(summarize_text))
#let's begin
generate_summary("msft.txt",2)
